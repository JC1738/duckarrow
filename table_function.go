package main

/*
#cgo CFLAGS: -I${SRCDIR}/../duckdb-go-api -DDUCKDB_API_EXCLUDE_FUNCTIONS=1
#include <stdlib.h>
#include <duckdb.h>
#include <duckdb_go_extension.h>

// Callback wrappers - these call back into Go
void duckarrow_bind_wrapper(duckdb_bind_info info);
void duckarrow_init_wrapper(duckdb_init_info info);
void duckarrow_scan_wrapper(duckdb_function_info info, duckdb_data_chunk output);
void duckarrow_destroy_bind_data(void *data);
void duckarrow_destroy_init_data(void *data);
*/
import "C"
import (
	"context"
	"duckdb"
	"fmt"
	"main/internal/flight"
	"runtime"
	"runtime/cgo"
	"sync/atomic"
	"unsafe"

	"github.com/apache/arrow-adbc/go/adbc"
	"github.com/apache/arrow-go/v18/arrow"
	"github.com/apache/arrow-go/v18/arrow/array"
)

// BindData holds state from bind phase
type BindData struct {
	// Connection info
	Client   *flight.Client
	Config   flight.Config // Store config for pool release
	IsPooled bool          // If true, use ReleaseConnection; if false, use Client.Close()
	URI      string        // Original URI for deferred query execution

	// Deferred query construction (for projection pushdown)
	TableName  string   // Raw table name (not full query)
	AllColumns []string // All column names from schema (for projection mapping)
	Schema     *arrow.Schema

	// Query state (set in init phase with projection pushdown, or bind phase without)
	Stmt   adbc.Statement
	Reader array.RecordReader

	// Legacy field for backward compatibility
	Query string
}

// ScanState tracks scanning progress
type ScanState struct {
	CurrentBatch  arrow.RecordBatch
	BatchPosition int64
	Done          int32
}

//export duckarrow_bind_wrapper
func duckarrow_bind_wrapper(info C.duckdb_bind_info) {
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()

	// Get parameter count
	paramCount := C.duckdb_bind_get_parameter_count(info)

	if paramCount < 2 {
		// Backward compatibility: no parameters, return hardcoded test data
		bindHardcodedData(info)
		return
	}

	// Get URI and query parameters
	uriVal := C.duckdb_bind_get_parameter(info, 0)
	queryVal := C.duckdb_bind_get_parameter(info, 1)
	defer C.duckdb_destroy_value(&uriVal)
	defer C.duckdb_destroy_value(&queryVal)

	uriCStr := C.duckdb_get_varchar(uriVal)
	queryCStr := C.duckdb_get_varchar(queryVal)
	uri := C.GoString(uriCStr)
	query := C.GoString(queryCStr)
	C.duckdb_free(unsafe.Pointer(uriCStr))
	C.duckdb_free(unsafe.Pointer(queryCStr))

	// Extract table name from query (format: SELECT * FROM "tablename")
	// This is generated by the replacement scan
	// If extraction fails, the query is arbitrary SQL and we skip projection pushdown
	tableName := extractTableName(query)

	// Get credentials and settings from global config (set by duckarrow_configure)
	_, configUsername, configPassword, configSkipVerify := GetDuckArrowConfig()

	// Build config for connection pool
	cfg := flight.Config{
		URI:        uri,
		Username:   configUsername,
		Password:   configPassword,
		SkipVerify: configSkipVerify,
	}

	// Get connection from pool (or create new)
	ctx := context.Background()
	connResult, err := flight.GetConnection(ctx, cfg)
	if err != nil {
		duckdb.SetBindError(duckdb.BindInfo{Ptr: unsafe.Pointer(info)}, "connection failed: %v", err)
		return
	}

	// Determine which query to use for schema discovery
	// If we have a table name, use schema-only query for efficiency
	// Otherwise, use the original query (for arbitrary SQL)
	var schemaQuery string
	if tableName != "" {
		schemaQuery = buildSchemaQuery(tableName)
	} else {
		// For arbitrary SQL, we need to execute it to get the schema
		// We'll execute the full query in bind phase and store the result
		schemaQuery = query
	}

	result, err := connResult.Client.Query(ctx, schemaQuery)
	if err != nil {
		// Clean up connection based on whether it's pooled
		if connResult.IsPooled {
			flight.ReleaseConnection(cfg)
		} else {
			connResult.Client.Close()
		}
		duckdb.SetBindError(duckdb.BindInfo{Ptr: unsafe.Pointer(info)}, "query failed: %v", err)
		return
	}

	// Get schema and column names
	schema := result.Reader.Schema()
	allColumns := make([]string, len(schema.Fields()))
	for i, field := range schema.Fields() {
		allColumns[i] = field.Name
		colName := C.CString(field.Name)
		colType := arrowTypeToDuckDB(field.Type)
		C.duckdb_bind_add_result_column(info, colName, colType)
		C.duckdb_destroy_logical_type(&colType)
		C.free(unsafe.Pointer(colName))
	}

	// For table queries with projection pushdown, release schema resources
	// For arbitrary SQL, keep the result since we can't re-execute with projection
	var bindData *BindData
	if tableName != "" {
		// Release schema query resources - we'll re-execute with projected columns
		result.Reader.Release()
		result.Stmt.Close()
		bindData = &BindData{
			Client:     connResult.Client,
			Config:     cfg,
			IsPooled:   connResult.IsPooled,
			URI:        uri,
			TableName:  tableName,
			AllColumns: allColumns,
			Schema:     schema,
			Query:      query,
			// Stmt and Reader will be set in init phase
		}
	} else {
		// Arbitrary SQL - keep the result, no projection pushdown
		bindData = &BindData{
			Client:     connResult.Client,
			Config:     cfg,
			IsPooled:   connResult.IsPooled,
			URI:        uri,
			TableName:  "", // Empty means no projection pushdown
			AllColumns: allColumns,
			Schema:     schema,
			Query:      query,
			Stmt:       result.Stmt,
			Reader:     result.Reader,
		}
	}
	handle := cgo.NewHandle(bindData)
	C.duckdb_bind_set_bind_data(info, unsafe.Pointer(handle),
		C.duckdb_delete_callback_t(C.duckarrow_destroy_bind_data))
}

// bindHardcodedData provides backward compatibility for no-parameter calls
func bindHardcodedData(info C.duckdb_bind_info) {
	colName1 := C.CString("id")
	colName2 := C.CString("name")
	defer C.free(unsafe.Pointer(colName1))
	defer C.free(unsafe.Pointer(colName2))

	varcharType := C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
	defer C.duckdb_destroy_logical_type(&varcharType)

	C.duckdb_bind_add_result_column(info, colName1, varcharType)
	C.duckdb_bind_add_result_column(info, colName2, varcharType)

	bindData := &BindData{Query: "hardcoded"}
	handle := cgo.NewHandle(bindData)
	C.duckdb_bind_set_bind_data(info, unsafe.Pointer(handle),
		C.duckdb_delete_callback_t(C.duckarrow_destroy_bind_data))
}

// arrowTypeToDuckDB converts Arrow types to DuckDB logical types
func arrowTypeToDuckDB(t arrow.DataType) C.duckdb_logical_type {
	switch t.ID() {
	case arrow.STRING, arrow.LARGE_STRING:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
	case arrow.INT64:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_BIGINT)
	case arrow.INT32:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_INTEGER)
	case arrow.INT16:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_SMALLINT)
	case arrow.INT8:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_TINYINT)
	case arrow.UINT64:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_UBIGINT)
	case arrow.UINT32:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_UINTEGER)
	case arrow.UINT16:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_USMALLINT)
	case arrow.UINT8:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_UTINYINT)
	case arrow.FLOAT64:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_DOUBLE)
	case arrow.FLOAT32:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_FLOAT)
	case arrow.BOOL:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_BOOLEAN)
	case arrow.TIMESTAMP:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_TIMESTAMP)
	case arrow.DATE32, arrow.DATE64:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_DATE)
	case arrow.TIME32, arrow.TIME64:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_TIME)
	case arrow.BINARY, arrow.LARGE_BINARY:
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_BLOB)
	case arrow.FIXED_SIZE_BINARY:
		// UUIDs are typically 16-byte FixedSizeBinary, return as VARCHAR
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
	case arrow.DECIMAL128:
		dt := t.(*arrow.Decimal128Type)
		// DuckDB max precision is 38, Arrow can be up to 38
		width := uint8(min(dt.Precision, 38))
		scale := uint8(dt.Scale)
		return C.duckdb_create_decimal_type(C.uint8_t(width), C.uint8_t(scale))
	case arrow.DECIMAL256:
		// DECIMAL256 can exceed 38 digits - truncate to DuckDB max
		dt := t.(*arrow.Decimal256Type)
		width := uint8(min(dt.Precision, 38))
		scale := uint8(dt.Scale)
		return C.duckdb_create_decimal_type(C.uint8_t(width), C.uint8_t(scale))
	case arrow.STRUCT:
		structType := t.(*arrow.StructType)
		numFields := len(structType.Fields())
		if numFields == 0 {
			// Empty struct - return VARCHAR as fallback
			return C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
		}
		memberTypes := make([]C.duckdb_logical_type, numFields)
		memberNames := make([]*C.char, numFields)
		for i, field := range structType.Fields() {
			memberTypes[i] = arrowTypeToDuckDB(field.Type)
			memberNames[i] = C.CString(field.Name)
		}
		result := C.duckdb_create_struct_type(&memberTypes[0], &memberNames[0], C.idx_t(numFields))
		// Clean up
		for i := range memberTypes {
			C.duckdb_destroy_logical_type(&memberTypes[i])
			C.free(unsafe.Pointer(memberNames[i]))
		}
		return result
	case arrow.LIST:
		listType := t.(*arrow.ListType)
		childType := arrowTypeToDuckDB(listType.Elem())
		result := C.duckdb_create_list_type(childType)
		C.duckdb_destroy_logical_type(&childType)
		return result
	case arrow.LARGE_LIST:
		listType := t.(*arrow.LargeListType)
		childType := arrowTypeToDuckDB(listType.Elem())
		result := C.duckdb_create_list_type(childType)
		C.duckdb_destroy_logical_type(&childType)
		return result
	case arrow.MAP:
		mapType := t.(*arrow.MapType)
		keyType := arrowTypeToDuckDB(mapType.KeyType())
		valueType := arrowTypeToDuckDB(mapType.ItemType())
		result := C.duckdb_create_map_type(keyType, valueType)
		C.duckdb_destroy_logical_type(&keyType)
		C.duckdb_destroy_logical_type(&valueType)
		return result
	default:
		// Fallback to VARCHAR for unknown types
		return C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
	}
}

//export duckarrow_init_wrapper
func duckarrow_init_wrapper(info C.duckdb_init_info) {
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()

	// Initialize scan state
	state := &ScanState{}
	stateHandle := cgo.NewHandle(state)
	C.duckdb_init_set_init_data(info, unsafe.Pointer(stateHandle),
		C.duckdb_delete_callback_t(C.duckarrow_destroy_init_data))

	// Get bind data to execute the projected query
	bindPtr := C.duckdb_init_get_bind_data(info)
	if bindPtr == nil {
		return // Hardcoded data mode - no real query
	}
	bindHandle := cgo.Handle(uintptr(bindPtr))
	bindData, ok := bindHandle.Value().(*BindData)
	if !ok || bindData.TableName == "" {
		// Early return for:
		// 1. Hardcoded data mode (invalid bindData)
		// 2. Arbitrary SQL queries (TableName empty, but Reader already set in bind phase)
		// In both cases, no projection pushdown is needed/possible
		return
	}

	// Extract projected columns from DuckDB
	// DuckDB tells us which columns are actually needed
	columnCount := C.duckdb_init_get_column_count(info)
	projectedColumns := make([]string, columnCount)
	for i := C.idx_t(0); i < columnCount; i++ {
		colIdx := C.duckdb_init_get_column_index(info, i)
		if int(colIdx) >= len(bindData.AllColumns) {
			duckdb.SetInitError(duckdb.InitInfo{Ptr: unsafe.Pointer(info)}, "column index %d out of range (max %d)", colIdx, len(bindData.AllColumns)-1)
			return
		}
		projectedColumns[i] = bindData.AllColumns[colIdx]
	}

	// Build optimized query with only the needed columns
	query := buildProjectedQuery(bindData.TableName, projectedColumns)

	// Execute the actual data query
	ctx := context.Background()
	result, err := bindData.Client.Query(ctx, query)
	if err != nil {
		duckdb.SetInitError(duckdb.InitInfo{Ptr: unsafe.Pointer(info)}, "query execution failed: %v", err)
		return
	}

	// Store the query result for the scan phase
	bindData.Stmt = result.Stmt
	bindData.Reader = result.Reader
}

//export duckarrow_scan_wrapper
func duckarrow_scan_wrapper(info C.duckdb_function_info, output C.duckdb_data_chunk) {
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()

	// Get bind data
	bindPtr := C.duckdb_function_get_bind_data(info)
	bindHandle := cgo.Handle(uintptr(bindPtr))
	bindData, ok := bindHandle.Value().(*BindData)
	if !ok {
		duckdb.SetFunctionError(duckdb.FunctionInfo{Ptr: unsafe.Pointer(info)}, "internal error: invalid bind data type")
		return
	}

	// Get scan state
	statePtr := C.duckdb_function_get_init_data(info)
	stateHandle := cgo.Handle(uintptr(statePtr))
	state, ok := stateHandle.Value().(*ScanState)
	if !ok {
		duckdb.SetFunctionError(duckdb.FunctionInfo{Ptr: unsafe.Pointer(info)}, "internal error: invalid scan state type")
		return
	}

	// Check if this is hardcoded test data mode (no Flight SQL connection)
	if bindData.Reader == nil {
		scanHardcodedData(info, output, state)
		return
	}

	// Scan Arrow data from Flight SQL
	scanArrowData(info, output, bindData, state)
}

// scanHardcodedData returns hardcoded test data for backward compatibility
func scanHardcodedData(info C.duckdb_function_info, output C.duckdb_data_chunk, state *ScanState) {
	if atomic.LoadInt32(&state.Done) == 1 {
		C.duckdb_data_chunk_set_size(output, 0)
		return
	}

	rows := []struct{ id, name string }{
		{"1", "Alice"},
		{"2", "Bob"},
		{"3", "Charlie"},
	}

	idVec := duckdb.DataChunkGetVector(duckdb.DataChunk{Ptr: unsafe.Pointer(output)}, 0)
	nameVec := duckdb.DataChunkGetVector(duckdb.DataChunk{Ptr: unsafe.Pointer(output)}, 1)

	for i, row := range rows {
		duckdb.AssignStringToVector(idVec, i, row.id)
		duckdb.AssignStringToVector(nameVec, i, row.name)
	}

	C.duckdb_data_chunk_set_size(output, C.idx_t(len(rows)))
	atomic.StoreInt32(&state.Done, 1)
}

// scanArrowData streams Arrow data from Flight SQL to DuckDB
func scanArrowData(info C.duckdb_function_info, output C.duckdb_data_chunk, bindData *BindData, state *ScanState) {
	if atomic.LoadInt32(&state.Done) == 1 {
		C.duckdb_data_chunk_set_size(output, 0)
		return
	}

	// Get next batch if needed
	if state.CurrentBatch == nil || state.BatchPosition >= state.CurrentBatch.NumRows() {
		// Release previous batch
		if state.CurrentBatch != nil {
			state.CurrentBatch.Release()
			state.CurrentBatch = nil
		}

		if !bindData.Reader.Next() {
			if err := bindData.Reader.Err(); err != nil {
				duckdb.SetFunctionError(duckdb.FunctionInfo{Ptr: unsafe.Pointer(info)}, "%v", err)
			}
			atomic.StoreInt32(&state.Done, 1)
			C.duckdb_data_chunk_set_size(output, 0)
			return
		}

		// Get new batch and retain it
		state.CurrentBatch = bindData.Reader.RecordBatch()
		state.CurrentBatch.Retain()
		state.BatchPosition = 0
	}

	// Calculate rows to emit (max 2048 per DuckDB chunk)
	const maxChunkSize = 2048
	remaining := state.CurrentBatch.NumRows() - state.BatchPosition
	rowsToEmit := int(remaining)
	if rowsToEmit > maxChunkSize {
		rowsToEmit = maxChunkSize
	}

	// Convert each column
	numCols := int(state.CurrentBatch.NumCols())
	for colIdx := 0; colIdx < numCols; colIdx++ {
		arrowCol := state.CurrentBatch.Column(colIdx)
		duckVec := duckdb.DataChunkGetVector(duckdb.DataChunk{Ptr: unsafe.Pointer(output)}, uint64(colIdx))

		if err := convertArrowToDuckDB(arrowCol, duckVec, int(state.BatchPosition), rowsToEmit); err != nil {
			duckdb.SetFunctionError(duckdb.FunctionInfo{Ptr: unsafe.Pointer(info)}, "convert col %d: %v", colIdx, err)
			return
		}
	}

	state.BatchPosition += int64(rowsToEmit)
	C.duckdb_data_chunk_set_size(output, C.idx_t(rowsToEmit))
}

// convertArrowToDuckDB converts Arrow column data to DuckDB vector with proper types
func convertArrowToDuckDB(arrowCol arrow.Array, duckVec duckdb.Vector, offset, count int) error {
	// Ensure validity mask is writable
	duckdb.VectorEnsureValidityWritable(duckVec)
	validity := duckdb.VectorGetValidity(duckVec)

	// Handle type-specific conversion
	switch col := arrowCol.(type) {
	case *array.String:
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			duckdb.AssignStringToVector(duckVec, i, col.Value(srcIdx))
		}

	case *array.LargeString:
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			duckdb.AssignStringToVector(duckVec, i, col.Value(srcIdx))
		}

	case *array.Int64:
		ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.int64_t(col.Value(srcIdx))
		}

	case *array.Int32:
		ptr := (*C.int32_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.int32_t(col.Value(srcIdx))
		}

	case *array.Int16:
		ptr := (*C.int16_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.int16_t(col.Value(srcIdx))
		}

	case *array.Int8:
		ptr := (*C.int8_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.int8_t(col.Value(srcIdx))
		}

	case *array.Uint64:
		ptr := (*C.uint64_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.uint64_t(col.Value(srcIdx))
		}

	case *array.Uint32:
		ptr := (*C.uint32_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.uint32_t(col.Value(srcIdx))
		}

	case *array.Uint16:
		ptr := (*C.uint16_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.uint16_t(col.Value(srcIdx))
		}

	case *array.Uint8:
		ptr := (*C.uint8_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.uint8_t(col.Value(srcIdx))
		}

	case *array.Float64:
		ptr := (*C.double)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.double(col.Value(srcIdx))
		}

	case *array.Float32:
		ptr := (*C.float)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.float(col.Value(srcIdx))
		}

	case *array.Boolean:
		ptr := (*C.uint8_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			if col.Value(srcIdx) {
				data[i] = 1
			} else {
				data[i] = 0
			}
		}

	case *array.Timestamp:
		// DuckDB TIMESTAMP is microseconds since epoch
		unit := col.DataType().(*arrow.TimestampType).Unit
		ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			ts := col.Value(srcIdx)
			var micros int64
			switch unit {
			case arrow.Second:
				micros = int64(ts) * 1_000_000
			case arrow.Millisecond:
				micros = int64(ts) * 1_000
			case arrow.Microsecond:
				micros = int64(ts)
			case arrow.Nanosecond:
				micros = int64(ts) / 1_000
			}
			data[i] = C.int64_t(micros)
		}

	case *array.Date32:
		// DuckDB DATE is days since epoch
		ptr := (*C.int32_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data[i] = C.int32_t(col.Value(srcIdx))
		}

	case *array.Date64:
		// Date64 is milliseconds since epoch, convert to days for DuckDB
		ptr := (*C.int32_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			// Convert milliseconds to days
			msPerDay := int64(24 * 60 * 60 * 1000)
			data[i] = C.int32_t(col.Value(srcIdx) / arrow.Date64(msPerDay))
		}

	case *array.Time32:
		// DuckDB TIME is microseconds since midnight
		unit := col.DataType().(*arrow.Time32Type).Unit
		ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			t := col.Value(srcIdx)
			var micros int64
			switch unit {
			case arrow.Second:
				micros = int64(t) * 1_000_000
			case arrow.Millisecond:
				micros = int64(t) * 1_000
			default:
				micros = int64(t)
			}
			data[i] = C.int64_t(micros)
		}

	case *array.Time64:
		// DuckDB TIME is microseconds since midnight
		unit := col.DataType().(*arrow.Time64Type).Unit
		ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
		data := unsafe.Slice(ptr, count)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			t := col.Value(srcIdx)
			var micros int64
			switch unit {
			case arrow.Microsecond:
				micros = int64(t)
			case arrow.Nanosecond:
				micros = int64(t) / 1_000
			default:
				micros = int64(t)
			}
			data[i] = C.int64_t(micros)
		}

	case *array.Binary:
		// Convert to BLOB
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			duckdb.AssignBytesToVector(duckVec, i, col.Value(srcIdx))
		}

	case *array.LargeBinary:
		// Convert to BLOB
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			duckdb.AssignBytesToVector(duckVec, i, col.Value(srcIdx))
		}

	case *array.FixedSizeBinary:
		// Convert to VARCHAR (UUIDs are typically 16-byte FixedSizeBinary)
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}
			data := col.Value(srcIdx)
			var val string
			if len(data) == 16 {
				// Format as UUID
				val = fmt.Sprintf("%08x-%04x-%04x-%04x-%012x",
					data[0:4], data[4:6], data[6:8], data[8:10], data[10:16])
			} else {
				val = fmt.Sprintf("%x", data)
			}
			duckdb.AssignStringToVector(duckVec, i, val)
		}

	case *array.Decimal128:
		// DuckDB uses different internal storage based on precision:
		// 1-4: INT16, 5-9: INT32, 10-18: INT64, 19-38: HUGEINT
		decType := col.DataType().(*arrow.Decimal128Type)
		precision := decType.Precision

		switch {
		case precision <= 4:
			ptr := (*C.int16_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				data[i] = C.int16_t(val.LowBits())
			}
		case precision <= 9:
			ptr := (*C.int32_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				data[i] = C.int32_t(val.LowBits())
			}
		case precision <= 18:
			ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				data[i] = C.int64_t(val.LowBits())
			}
		default: // 19-38
			ptr := (*C.duckdb_hugeint)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				data[i] = C.duckdb_hugeint{
					lower: C.uint64_t(val.LowBits()),
					upper: C.int64_t(val.HighBits()),
				}
			}
		}

	case *array.Decimal256:
		// DECIMAL256 - DuckDB max precision is 38, so we truncate to low 128 bits
		// DuckDB uses different internal storage based on precision:
		// 1-4: INT16, 5-9: INT32, 10-18: INT64, 19-38: HUGEINT
		decType := col.DataType().(*arrow.Decimal256Type)
		precision := int32(min(decType.Precision, 38))

		switch {
		case precision <= 4:
			ptr := (*C.int16_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				// Use low 64 bits of the 256-bit value
				arr := val.Array()
				data[i] = C.int16_t(arr[0])
			}
		case precision <= 9:
			ptr := (*C.int32_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				arr := val.Array()
				data[i] = C.int32_t(arr[0])
			}
		case precision <= 18:
			ptr := (*C.int64_t)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				arr := val.Array()
				data[i] = C.int64_t(arr[0])
			}
		default: // 19-38
			ptr := (*C.duckdb_hugeint)(duckdb.VectorGetData(duckVec))
			data := unsafe.Slice(ptr, count)
			for i := 0; i < count; i++ {
				srcIdx := offset + i
				if col.IsNull(srcIdx) {
					duckdb.ValiditySetRowInvalid(validity, uint64(i))
					continue
				}
				val := col.Value(srcIdx)
				arr := val.Array()
				// arr is [4]uint64, use low 128 bits (arr[0] is lower, arr[1] is upper)
				data[i] = C.duckdb_hugeint{
					lower: C.uint64_t(arr[0]),
					upper: C.int64_t(arr[1]),
				}
			}
		}

	case *array.Struct:
		// Convert STRUCT by recursively converting each field
		structType := col.DataType().(*arrow.StructType)
		numFields := structType.NumFields()

		// Set validity for struct-level nulls
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
			}
		}

		// Convert each field recursively
		for fieldIdx := 0; fieldIdx < numFields; fieldIdx++ {
			childArr := col.Field(fieldIdx)
			childVec := duckdb.StructVectorGetChild(duckVec, uint64(fieldIdx))
			if err := convertArrowToDuckDB(childArr, childVec, offset, count); err != nil {
				return fmt.Errorf("struct field %d: %w", fieldIdx, err)
			}
		}

	case *array.List:
		// Convert LIST with offset/length tracking
		offsets := col.Offsets()
		childArr := col.ListValues()
		childVec := duckdb.ListVectorGetChild(duckVec)

		// Get pointer to list entry data
		listEntryPtr := (*C.duckdb_list_entry)(duckdb.VectorGetData(duckVec))
		listEntries := unsafe.Slice(listEntryPtr, count)

		// Calculate the range of child elements we need
		// Arrow offsets are absolute positions in the child array
		firstChildIdx := offsets[offset]
		lastChildIdx := offsets[offset+count]
		totalChildElements := lastChildIdx - firstChildIdx

		// Reserve space in child vector
		if totalChildElements > 0 {
			duckdb.ListVectorReserve(duckVec, uint64(totalChildElements))
		}

		// Set up list entries with DuckDB-relative offsets
		var duckChildOffset uint64
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				listEntries[i] = C.duckdb_list_entry{offset: 0, length: 0}
				continue
			}

			start := offsets[srcIdx]
			end := offsets[srcIdx+1]
			length := end - start

			listEntries[i] = C.duckdb_list_entry{
				offset: C.idx_t(duckChildOffset),
				length: C.idx_t(length),
			}
			duckChildOffset += uint64(length)
		}

		// Convert all child elements in one batch
		if totalChildElements > 0 {
			if err := convertArrowToDuckDB(childArr, childVec, int(firstChildIdx), int(totalChildElements)); err != nil {
				return fmt.Errorf("list elements: %w", err)
			}
		}

		// Set the total child size
		duckdb.ListVectorSetSize(duckVec, uint64(totalChildElements))

	case *array.LargeList:
		// Convert LARGE_LIST (same as LIST but with int64 offsets)
		offsets := col.Offsets()
		childArr := col.ListValues()
		childVec := duckdb.ListVectorGetChild(duckVec)

		// Get pointer to list entry data
		listEntryPtr := (*C.duckdb_list_entry)(duckdb.VectorGetData(duckVec))
		listEntries := unsafe.Slice(listEntryPtr, count)

		// Calculate the range of child elements we need
		// Arrow offsets are absolute positions in the child array
		firstChildIdx := offsets[offset]
		lastChildIdx := offsets[offset+count]
		totalChildElements := lastChildIdx - firstChildIdx

		// Reserve space in child vector
		if totalChildElements > 0 {
			duckdb.ListVectorReserve(duckVec, uint64(totalChildElements))
		}

		// Set up list entries with DuckDB-relative offsets
		var duckChildOffset uint64
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				listEntries[i] = C.duckdb_list_entry{offset: 0, length: 0}
				continue
			}

			start := offsets[srcIdx]
			end := offsets[srcIdx+1]
			length := end - start

			listEntries[i] = C.duckdb_list_entry{
				offset: C.idx_t(duckChildOffset),
				length: C.idx_t(length),
			}
			duckChildOffset += uint64(length)
		}

		// Convert all child elements in one batch
		if totalChildElements > 0 {
			if err := convertArrowToDuckDB(childArr, childVec, int(firstChildIdx), int(totalChildElements)); err != nil {
				return fmt.Errorf("large list elements: %w", err)
			}
		}

		// Set the total child size
		duckdb.ListVectorSetSize(duckVec, uint64(totalChildElements))

	case *array.Map:
		// MAP is stored as LIST of STRUCT{key, value}
		// DuckDB MAP uses same internal structure as LIST
		offsets := col.Offsets()
		keys := col.Keys()
		items := col.Items()
		childVec := duckdb.ListVectorGetChild(duckVec)

		// Get pointer to list entry data
		listEntryPtr := (*C.duckdb_list_entry)(duckdb.VectorGetData(duckVec))
		listEntries := unsafe.Slice(listEntryPtr, count)

		// Calculate the range of entries we need
		// Arrow offsets are absolute positions in the keys/items arrays
		firstEntryIdx := offsets[offset]
		lastEntryIdx := offsets[offset+count]
		totalEntries := lastEntryIdx - firstEntryIdx

		// Reserve space
		if totalEntries > 0 {
			duckdb.ListVectorReserve(duckVec, uint64(totalEntries))
		}

		// Get child struct's key and value vectors
		// MAP child is a STRUCT{key, value}
		keyChildVec := duckdb.StructVectorGetChild(childVec, 0)
		valueChildVec := duckdb.StructVectorGetChild(childVec, 1)

		// Set up list entries with DuckDB-relative offsets
		var duckChildOffset uint64
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if col.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				listEntries[i] = C.duckdb_list_entry{offset: 0, length: 0}
				continue
			}

			start := offsets[srcIdx]
			end := offsets[srcIdx+1]
			length := end - start

			listEntries[i] = C.duckdb_list_entry{
				offset: C.idx_t(duckChildOffset),
				length: C.idx_t(length),
			}
			duckChildOffset += uint64(length)
		}

		// Convert all keys and values in one batch
		if totalEntries > 0 {
			if err := convertArrowToDuckDB(keys, keyChildVec, int(firstEntryIdx), int(totalEntries)); err != nil {
				return fmt.Errorf("map keys: %w", err)
			}
			if err := convertArrowToDuckDB(items, valueChildVec, int(firstEntryIdx), int(totalEntries)); err != nil {
				return fmt.Errorf("map values: %w", err)
			}
		}

		// Set the total child size
		duckdb.ListVectorSetSize(duckVec, uint64(totalEntries))

	default:
		// Fallback: convert to string using ValueStr if available
		for i := 0; i < count; i++ {
			srcIdx := offset + i
			if arrowCol.IsNull(srcIdx) {
				duckdb.ValiditySetRowInvalid(validity, uint64(i))
				continue
			}

			var val string
			if stringer, ok := arrowCol.(interface{ ValueStr(int) string }); ok {
				val = stringer.ValueStr(srcIdx)
			} else {
				val = fmt.Sprintf("%v", arrowCol.GetOneForMarshal(srcIdx))
			}

			duckdb.AssignStringToVector(duckVec, i, val)
		}
	}

	return nil
}

//export duckarrow_destroy_bind_data
func duckarrow_destroy_bind_data(data unsafe.Pointer) {
	if data == nil {
		return
	}
	handle := cgo.Handle(uintptr(data))
	bindData := handle.Value().(*BindData)

	// Clean up query resources (reader and statement)
	if bindData.Reader != nil {
		bindData.Reader.Release()
	}
	if bindData.Stmt != nil {
		bindData.Stmt.Close()
	}

	// Clean up connection based on whether it's pooled
	if bindData.Client != nil {
		if bindData.IsPooled {
			flight.ReleaseConnection(bindData.Config)
		} else {
			bindData.Client.Close()
		}
	}

	handle.Delete()
}

//export duckarrow_destroy_init_data
func duckarrow_destroy_init_data(data unsafe.Pointer) {
	if data == nil {
		return
	}
	handle := cgo.Handle(uintptr(data))
	state := handle.Value().(*ScanState)

	// Release current batch if any
	if state.CurrentBatch != nil {
		state.CurrentBatch.Release()
	}

	handle.Delete()
}

// RegisterDuckArrowQuery registers the duckarrow_query table function
func RegisterDuckArrowQuery(conn duckdb.Connection) duckdb.State {
	tableFunc := C.duckdb_create_table_function()
	defer C.duckdb_destroy_table_function(&tableFunc)

	name := C.CString("duckarrow_query")
	defer C.free(unsafe.Pointer(name))
	C.duckdb_table_function_set_name(tableFunc, name)

	// Add two VARCHAR parameters: uri and query
	varcharType := C.duckdb_create_logical_type(C.DUCKDB_TYPE_VARCHAR)
	C.duckdb_table_function_add_parameter(tableFunc, varcharType)
	C.duckdb_table_function_add_parameter(tableFunc, varcharType)
	C.duckdb_destroy_logical_type(&varcharType)

	// Enable projection pushdown - allows DuckDB to tell us which columns are needed
	C.duckdb_table_function_supports_projection_pushdown(tableFunc, true)

	// Set callbacks
	C.duckdb_table_function_set_bind(tableFunc,
		C.duckdb_table_function_bind_t(C.duckarrow_bind_wrapper))
	C.duckdb_table_function_set_init(tableFunc,
		C.duckdb_table_function_init_t(C.duckarrow_init_wrapper))
	C.duckdb_table_function_set_function(tableFunc,
		C.duckdb_table_function_t(C.duckarrow_scan_wrapper))

	return duckdb.State(C.duckdb_register_table_function(
		C.duckdb_connection(conn.Ptr), tableFunc))
}
